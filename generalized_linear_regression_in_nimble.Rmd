---
title: "Generalized Linear Regression in NIMBLE"
author: "Adam B. Smith"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
.libPaths('C:/ecology/Drive/R/libraries') # for Adam's computer only
```

This tutorial walks you through how to do generalized linear regression. Specifically, we will explore logistic regression (where the response is 1s and 0s). We'll use simulated data for which we know the "true" answers.

Again, the [online manual](https://r-nimble.org/html_manual/cha-welcome-nimble.html) for `nimble` is very useful. The [main webpage](https://r-nimble.org/) for `nimble` also links to a host of useful information.

## Setup
```{r, }
library(nimble) # the workhorse
library(coda) # for model diagnostics
library(bayesplot) # for graphing
```

## Create some fake data
We need to create a response vector that has just 1s and 0s. We'll do this by simulating some data, `y`, that responds linearly to `x`. Values of `y` will follow a normal distribution. We'll then use the inverse logit transform to convert `y` to the range (0, 1) (like a probability). Finally, we'll impose a probabilistic threshold to convert these values to 1s and 0s.

```{r}
N <- 100 # number of data points
x <- rnorm(N) # predictor
beta0 <- 1 # intercept
beta1 <- 1.2 # slope
sigma <- 0.8 # variation around trendline

noise <- rnorm(N, mean = 0, sd = sigma)
y <- beta0 + beta1 * x + noise

# function for inverse logit
invlogit <- function(x) exp(x) / (1 + exp(x))

# convert to range of (0, 1)
ytrans <- invlogit(y)

# convert to 0/1 probabilistically
ybinary <- as.integer(runif(N) < ytrans)
```

Let's look at the data through the transformation process.
```{r, fig.width=7.5, fig.height=3.5}
par(mfrow = c(1, 3))

# x vs y
bg <- ifelse(ybinary == 1, "chartreuse", "red")
plot(x, y, pch = 21, bg = bg, main = 'x vs y')

legend('bottomright', legend = c(1, 0), pt.bg = c('chartreuse', 'red'), pch = 21)

# x vs inverse-logit transform of y
plot(x, ytrans, pch = 21, bg = bg, main = 'x vs invlogit(y)')

# x vs binary y
plot(x, ybinary, pch = 21, bg = bg, main = 'x vs binary')
```

Finally, we'll rescale our predictor (`x`) to have a 0 mean and unit variance.
```{r}
x_scaled <- scale(x) # output is a 1-column matrix
x_scaled <- c(x_scaled) # convert back to a vector
```

## Organize data into formats needed by `nimble`
```{r}
data <- list(ybinary = ybinary)

constants <- list(
	x_scaled = x_scaled,
	N = N
)

# two sets of initialization values, one for correct and one for incorrect model
inits <- list(
	beta0_hat = 0,
	beta1_hat = 0,
	y_est = ybinary
)
```

## What we're about to do
If we were interested in the relationship between `y` and `x` (neither of them transformed), we could use a linear model like this:

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

where, together, all the $\epsilon$s for all $i$ follow a normal distribution with a mean of 0 and standard deviation of $\sigma$. Ergo,

$$y = \beta_0 + \beta_1 x + N(0, \sigma)$$.

We've learned that this can be written like:

$$y \sim N(\beta_0 + \beta_1 x, \sigma)$$

meaning that`y` has a normal distribution that has a mean given by the estimated regression line.

For binary (logistic) regression, though, we have 0s and 1s, which cannot be described by a normal distribution. Rather, we use the Bernoulli distribution, which returns 0 or 1, and takes as an argument $\psi$, the probability that the value is 1. A Bernoulli distribution is essentially a single coin toss that returns heads or tails (or, equivalently, a binomial distribution with just one trial). So, we have:

$$ybinary_i \sim Bern(\psi_i)$$

We can now write an equation for $\psi$, as a function of things we think drive the probability of getting a 1 or 0.

$$logit(\psi) = \beta_0 + \beta_1 x$$

Note that:
* We use the `logit()` function to transform $\psi$, which has the range (0, 1) to unbounded values (i.e., between negative and positive infinity), just like a normal distribution.

* The right hand side is our linear regression equation from before. We're positing that the probability of getting a 1 or 0 depends on `x` and the $\beta$ coefficients.

* We don't include the $N(0, \sigma)$ part in the linear predictor. That's because the randomness is taken account of by the $Bern()$ distribution function.

NIMBLE has functions for the Bernoulli distribution and for the logit transform.

## Writing the `nimble` model
In addition to specifying our model, we will also ask NIMBLE to do a little extra, interesting work. Let's say we want to find the mean value of `ybinary`. This is easy to do using `mean(ybinary)`, but let's also say we want to calculate the mean of the values of `ybinary` that NIMBLE *thinks* the values should be, based on its model of `ybinary`. We can then compare `mean(ybinary)` with NIMBLE's estimate of the mean of `ybinary`.

```{r}
code <- nimbleCode({

	# likelihood of each datum
	for (i in 1:N) {

		# note the "sd = " part!
		ybinary[i] ~ dbern(psi[i])
		logit(psi[i]) <- beta0_hat + beta1_hat * x_scaled[i]

	}

	# likelihoods of priors
	# beta0_hat and beta1_hat: "broad" normals
	# sigma_hat: uniform >0 (after Gelman 2006 Bayesian Analysis)

	beta0_hat ~ dnorm(0, sd = 10)
	beta1_hat ~ dnorm(0, sd = 10)

	# simulate the data and calculate an auxillary variable
	# this does not affect the likelihood
	for (j in 1:N) {
		y_est[j] ~ dbern(psi[j])
	}
    y_est_mean <- mean(y_est[1:N])

})
```

## Process the model code
First, check the code for errors and do some other arcane things.

```{r}
# correct model
model <- nimbleModel(
	code = code, # our model
	constants = constants, # constants
	data = data, # data
	inits = inits, # initialization values
	check = TRUE, # any errors?
	calculate = FALSE
)

model$initializeInfo()

model$calculate()
```

Note that the starting log-likelihood (given the initialization values we used) is smaller for the incorrect model, which is telling.

We now create an MCMC "configuration" for the model. We also need to tell it what variables to keep track of.

```{r}
# corect model
monitors <- c('beta0_hat', 'beta1_hat', 'y_est_mean')

conf <- configureMCMC(
	model,
	monitors = monitors,
	print = TRUE,
	enableWAIC = FALSE # Watanabe's AIC
)
```

Lastly, we "build" and compile the model.

```{r}
build <- buildMCMC(conf)
compiled <- compileNimble(model, build)
```

## Run the MCMC sampler!

```{r}
chains <- runMCMC(
	compiled$build,
	niter = 1100, # iterations
	nburnin = 100, # burn-in
	thin = 1, # thinning rate
	nchains = 4, # number of chains
	inits = inits, # initialization values
	progressBar = TRUE,
	samplesAsCodaMCMC = TRUE, # for using coda package for plots
	summary = TRUE, # calculate summaries across chains
	WAIC = FALSE, # Watanabe's AIC
	perChainWAIC = FALSE
)
```

# Examine output
Let's look at the summary of our estimates:
```{r}
chains$summary$all.chains

# correct values
c(beta0 = beta0, beta1 = beta1, y_mean = mean(y))
```

These are our estimates for the parameters of interest. 

# Model diagnostics

## Chain convergence
First, calculate the the Gelman-Rubin diagnostic values for assessing convergence of our chains:
```{r}
gelman.diag(chains$samples)
```

In my run, these are all <1.1 so look OK.

## Chain mixing, convergence, and parameter estimates

Now, let's confirm what we saw in the G-R values by examining the trace plots and parameter estimates, first for the correct model.
```{r, fig.width=7.5}
mcmc_combo(chains$samples)
```

The chains look well-mixed in the trace plots (rihgt column), but the "lumpy" appearance of the posterior (the plots on the left) suggests that we should probably run the model for more iterations. But we'll live with this for now.

### The easier way
Note that we could have gotten a logistic regression simply by using `glm()` to do a "standard" generalized linear regression:
```{r}
model <- glm(ybinary ~ x_scaled, family = binomial())
coefficients(model)
```

*Finis!*
