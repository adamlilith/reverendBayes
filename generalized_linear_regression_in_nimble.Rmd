---
title: "Generalized Linear Regression in NIMBLE"
author: "Adam B. Smith"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
.libPaths('C:/ecology/Drive/R/libraries') # for Adam's computer only
```

This tutorial walks you through how to do generalized linear regression. Specifically, we will explore logistic regression (where the response is 1s and 0s). We'll use simulated data for which we know the "true" answers.  We'll also compare a "correct" model that uses the form of the function we used to generate the data, and an "incorrect" model that uses a spurious, extra term.

Again, the [online manual](https://r-nimble.org/html_manual/cha-welcome-nimble.html) for `nimble` is very useful. The [main webpage](https://r-nimble.org/) for `nimble` also links to a host of useful information.

## Setup
```{r, }
library(nimble) # the workhorse
library(coda) # for model diagnostics
library(bayesplot) # for graphing
```

## Create some fake data
We need to create a response vector that has just 1s and 0s. We'll do this by simulating some data, `y`, that responds linearly to `x`. Values of `y` will follow a normal distribution. We'll then use the inverse logit transform to convert `y` to the range (0, 1) (like a probability). Finally, we'll impose a probabilistic threshold to convert these values to 1s and 0s.

```{r}
N <- 100 # number of data points
x <- rnorm(N) # predictor
beta0 <- 1 # intercept
beta1 <- 1.2 # slope
sigma <- 0.8 # variation around trendline

noise <- rnorm(N, mean = 0, sd = sigma)
y <- beta0 + beta1 * x + noise

# function for inverse logit
invlogit <- function(x) exp(x) / (1 + exp(x))

# convert to range of (0, 1)
ytrans <- invlogit(y)

# convert to 0/1 probabilistically
ybinary <- as.integer(runif(N) < ytrans)
```

Let's look at the data through the transformation process.
```{r, fig.width=7.5, fig.height=3.5}
par(mfrow = c(1, 3))

# x vs y
bg <- ifelse(ybinary == 1, "chartreuse", "red")
plot(x, y, pch = 21, bg = bg, main = 'x vs y')

legend('bottomright', legend = c(1, 0), pt.bg = c('chartreuse', 'red'), pch = 21)

# x vs inverse-logit transform of y
plot(x, ytrans, pch = 21, bg = bg, main = 'x vs invlogit(y)')

# x vs binary y
plot(x, ybinary, pch = 21, bg = bg, main = 'x vs binary')
```

Finally, we'll rescale our predictor (`x`) to have a 0 mean and unit variance.
```{r}
x_scaled <- scale(x) # output is a 1-column matrix
x_scaled <- c(x_scaled) # convert back to a vector
```

## Organize data into formats needed by `nimble`
```{r}
data <- list(ybinary = ybinary)

constants <- list(
	x_scaled = x_scaled,
	N = N
)

# two sets of initialization values, one for correct and one for incorrect model
inits_correct <- list(
	beta0_hat = 0,
	beta1_hat = 0,
	y_est = ybinary
)

inits_incorrect <- list(
	beta0_hat = 0,
	beta1_hat = 0,
	beta2_hat = -1,
	y_est = ybinary
)
```

## What we're about to do
If we were interested in the relationship between `y` and `x` (neither of them transformed), we could use a linear model like this:

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

where, together, all the $\epsilon$s for all $i$ follow a normal distribution with a mean of 0 and standard deviation of $\sigma$. Ergo,

$$y = \beta_0 + \beta_1 x + N(0, \sigma)$$.

We've learned that this can be written like:

$$y \sim N(\beta_0 + \beta_1 x, \sigma)$$

meaning that`y` has a normal distribution that has a mean given by the estimated regression line.

For binary (logistic) regression, though, we have 0s and 1s, which cannot be described by a normal distribution. Rather, we use the Bernoulli distribution, which returns 0 or 1, and takes as an argument $\psi$, the probability that the value is 1. A Bernoulli distribution is essentially a single coin toss that returns heads or tails (or, equivalently, a binomial distribution with just one trial). So, we have:

$$ybinary_i \sim Bern(\psi_i)$$

We can now write an equation for $\psi$, as a function of things we think drive the probability of getting a 1 or 0.

$$logit(\psi) = \beta_0 + \beta_1 x$$

Note that:
* We use the `logit()` function to transform $\psi$, which has the range (0, 1) to unbounded values (i.e., between negative and positive infinity), just like a normal distribution.

* The right hand side is our linear regression equation from before. We're positing that the probability of getting a 1 or 0 depends on `x` and the $\beta$ coefficients.

* We don't include the $N(0, \sigma)$ part in the linear predictor. That's because the randomness is taken account of by the $Bern()$ distribution function.

NIMBLE has functions for the Bernoulli distribution and for the logit transform.

## Writing the `nimble` model
In addition to specifying our model, we will also ask NIMBLE to do a little extra, interesting work to help us assess which of our models is the correct one. Let's say we want to find the mean value of `ybinary`. This is easy to do using `mean(ybinary)`, but let's say we want to calculate the mean of the values of `ybinary` that NIMBLE *thinks* the values should be, based on its model of `ybinary`. We can then compare `mean(ybinary)` with NIMBLE's estimate of the mean of `ybinary`.

This is a kind of check we can do to see if we correctly specified the model. There are much more sophisticated ways of doing this (e.g., simulating all the `ybinary`s).

Below, the first model is "correct" because it says that the relationship between `y` (untransformed) is just a linear function of `x`. The second one is incorrect, because it adds a squared term which we did not include when we generated the data.

```{r}
code_correct <- nimbleCode({

	# likelihood of each datum
	for (i in 1:N) {

		# note the "sd = " part!
		ybinary[i] ~ dbern(psi[i])
		logit(psi[i]) <- beta0_hat + beta1_hat * x_scaled[i]

	}

	# likelihoods of priors
	# beta0_hat and beta1_hat: "broad" normals
	# sigma_hat: uniform >0 (after Gelman 2006 Bayesian Analysis)

	beta0_hat ~ dnorm(0, sd = 10)
	beta1_hat ~ dnorm(0, sd = 10)

	# simulate the data and calculate an auxillary variable
	# this does not affect the likelihood
	for (j in 1:N) {
		y_est[j] ~ dbern(psi[j])
	}
    y_est_mean <- mean(y_est[1:N])

})

code_incorrect <- nimbleCode({

    # likelihood of each datum
    for (i in 1:N) {
        # note the "sd = " part!
        ybinary[i] ~ dbern(psi[i])
        logit(psi[i]) <- beta0_hat + beta1_hat * x_scaled[i] + beta2_hat * (x_scaled[i])^2
    }

    # likelihoods of priors
    # beta0_hat and beta1_hat: "broad" normals
    # sigma_hat: uniform >0 (after Gelman 2006 Bayesian Analysis)

    beta0_hat ~ dnorm(0, sd = 10)
    beta1_hat ~ dnorm(0, sd = 10)
    beta2_hat ~ dnorm(0, sd = 10)

	# simulate the data and calculate an auxillary variable
	# this does not affect the likelihood
	for (j in 1:N) {
		y_est[j] ~ dbern(psi[j])
	}
    y_est_mean <- mean(y_est[1:N])

})
```

## Process the model code
First, check the code for errors and do some other arcane things.

```{r}
# correct model
model_correct <- nimbleModel(
	code = code_correct, # our model
	constants = constants, # constants
	data = data, # data
	inits = inits_correct, # initialization values
	check = TRUE, # any errors?
	calculate = FALSE
)

model_correct$initializeInfo()

model_correct$calculate()

# incorrect model
model_incorrect <- nimbleModel(
	code = code_incorrect, # our model
	constants = constants, # constants
	data = data, # data
	inits = inits_incorrect, # initialization values
	check = TRUE, # any errors?
	calculate = FALSE
)

model_incorrect$initializeInfo()

model_incorrect$calculate()
```

Note that the starting log-likelihood (given the initialization values we used) is smaller for the incorrect model, which is telling.

We now create an MCMC "configuration" for the model. We also need to tell it what variables to keep track of.

```{r}
# corect model
monitors_correct <- c('beta0_hat', 'beta1_hat', 'y_est_mean')

conf_correct <- configureMCMC(
	model_correct,
	monitors = monitors_correct,
	print = TRUE,
	enableWAIC = FALSE # Watanabe's AIC
)

# incorrect model
monitors_incorrect <- c('beta0_hat', 'beta1_hat', 'beta2_hat', 'y_est_mean')

conf_incorrect <- configureMCMC(
	model_incorrect,
	monitors = monitors_incorrect,
	print = TRUE,
	enableWAIC = FALSE # Watanabe's AIC
)
```

Lastly, we "build" and compile the model.

```{r}
build_correct <- buildMCMC(conf_correct)
build_incorrect <- buildMCMC(conf_incorrect)

compiled_correct <- compileNimble(model_correct, build_correct)
compiled_incorrect <- compileNimble(model_incorrect, build_incorrect)
```

## Run the MCMC sampler!

```{r}
chains_correct <- runMCMC(
	compiled_correct$build,
	niter = 1100, # iterations
	nburnin = 100, # burn-in
	thin = 1, # thinning rate
	nchains = 4, # number of chains
	inits = inits_correct, # initialization values
	progressBar = TRUE,
	samplesAsCodaMCMC = TRUE, # for using coda package for plots
	summary = TRUE, # calculate summaries across chains
	WAIC = FALSE, # Watanabe's AIC
	perChainWAIC = FALSE
)

chains_incorrect <- runMCMC(
	compiled_incorrect$build,
	niter = 1100, # iterations
	nburnin = 100, # burn-in
	thin = 1, # thinning rate
	nchains = 4, # number of chains
	inits = inits_incorrect, # initialization values
	progressBar = TRUE,
	samplesAsCodaMCMC = TRUE, # for using coda package for plots
	summary = TRUE, # calculate summaries across chains
	WAIC = FALSE, # Watanabe's AIC
	perChainWAIC = FALSE
)
```

# Examine output
Let's look at the summary of our estimates:
```{r}
chains_correct$summary$all.chains
chains_incorrect$summary$all.chains

# correct values
c(beta0 = beta0, beta1 = beta1, y_mean = mean(y))
```

These are our estimates for the parameters of interest. 

# Model diagnostics

## Chain convergence
First, calculate the the Gelman-Rubin diagnostic values for assessing convergence of our chains:
```{r}
gelman.diag(chains_correct$samples)
gelman.diag(chains_incorrect$samples)
```

In my run, these are all <1.1 so look OK.

## Chain mixing, convergence, and parameter estimates

Now, let's confirm what we saw in the G-R values by examining the trace plots and parameter estimates, first for the correct model.
```{r, fig.width=7.5}
mcmc_combo(chains_correct$samples)
```

Compare these to the incorrect model.
```{r, fig.width=7.5}
mcmc_combo(chains_incorrect$samples)
```

You can probably see that the chain estimates for the incorrect model seem less well-mixed than those for the correct model. This isn't necessarily a tell-tale sign of model misspecification. The incorrect model is more complex, so it is likely to take more time to mix sufficiently.

What about parameter estimates? They distributions are really very similar, and there's no clear winner. Our simulated version of `ybinary`, and its mean, also does not indicate which model is the correct one because both do a good job of recreating the data. Part of the reason is because the squared term (the `beta2_hat` coefficient) in the incorrect model was correctly estimated to be ~0, on average. Thus, it had little effect on simulated values of `ybinary`.

All the same, we get very equally-performing models. Given that, based on the principle of parsimony, we should probably use the simpler model.

### The easier way
Note that we could have gotten a logistic regression simply by using `glm()` to do a "standard" generalized linear regression:
```{r}
model <- glm(ybinary ~ x_scaled, family = binomial())
coefficients(model)
```

*Finis!*
