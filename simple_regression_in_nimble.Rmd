---
title: "Simple Regression in NIMBLE"
author: "Adam B. Smith"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
.libPaths('C:/ecology/Drive/R/libraries') # for Adam's computer only
```

This tutorial walks you through how to do a simple  Bayesian linear regression using the **`nimble`** package for **R**. linear regression using the nimble package for R. It also explores basic model diagnostics. We’ll try to estimate the “true” values we use to generate the data using simple linear regression. We will also explores basic model diagnostics. 

Note that there is an [online manual](https://r-nimble.org/html_manual/cha-welcome-nimble.html) for `nimble`. The [main webpage](https://r-nimble.org/) for `nimmble` also links to a host of useful infrmation.

## Setup
```{r, }
library(nimble) # the workhorse
library(coda) # for model diagnostics
library(bayesplot) # for graphing
```

## Create some fake data
```{r}
N <- 100 # number of data points
x <- rnorm(N) # predictor
beta0 <- 1 # intercept
beta1 <- 1.2 # slope
sigma <- 0.8 # variation around trendline

noise <- rnorm(N, mean = 0, sd = sigma)
y <- beta0 + beta1 * x + noise

plot(x, y)
```

## Rescaling predictors
MCMC and other apgorithms often suffer issues when predictors have distributions that vary widely. For example, total annual precipitation varies 0 to 7000 mm across the lower 48 US States, whereas mean annual temperature only varies from about -12 to 28 deg C. Precipitation has a much wider range than temperature, and this can cause issues if a model were to be used to find, say, a relationship between temperature, precipitation, and primary productivity.

To obviate tehse issues, we center and scale the predictors by subtracting their means and dividing by their standard deviations. This can be done quickly in **`R`** using the `scale()` function:

```{r}
x <- scale(x) # output is a 1-column matrix
x <- c(x) # convert back to a vector
```

Note that if we want to plot the model estimates against the original values of the predictors, we may need to unscale the predictors first.

## Organize data into formats needed by `nimble`
Data is supplied to `nimble` using `lists`:

* "Data" list: Supplies values that were measured. The distinction between data and "constants" (the next list), is that data have distributions so, in theory, could have been different values (i.e., if you were to repeat your sampling again).

* "Constants" list: This list has values that do not change. For example, the number of data points or number of statistical groups is a constant. It can also include predictor variables (which are assumed to be "fixed".) However, in some models, even the predictors are assumed to have error, so have distributions and therefore should go into the "data" list.

* "Initialization" list: This list is optional, but **`nimble`** works much more reliably if you provide some initial starting values for each parameter. Initialization values should chosen so that they are plausible values. Otherwise, the MCMC sampler could get stuck in the wrong part of parameter space. One way to choose good examples is to use a frequentist model to estimate values (e.g., we could use `lm()` to get goo starting values for the slope, intercept, and standard deviation). Typically, the frequentist model will be simpler than the Bayesian model (e.g., it might ignore sampling error).

```{r}
data <- list(y = y)

constants <- list(
	x = x,
	N = N
)

# note use of "_hat"
inits <- list(
	beta0_hat = 0,
	beta1_hat = 0,
	sigma_hat = 1
)
```

In the equations below, we'll be using the variables `beta0_hat`, `beta1_hat`, and `sigma_hat` to distinguish them from the values we defined to simulate "reality" in the steps above.

## What we're about to do
You are supremely familiar with the slope-intercept formula for a line:

$$y = \beta_0 + \beta_1 x$$

Assume that we've constructed a line in the plane defined by `x` and `y`. Now, the equation for the location of data point `i` is:

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

where $\epsilon_i$ is assumed to be a value drawn from a normal distribution with a mean of 0 and a standard deviation of $\sigma$. This means that the data has a mean of $y = \beta_0 + \beta_1 x$, and an "error" around this of $N(0, \sigma)$ (a normal distribution with a mean of 0 and standard deviation of $\sigma$).

This is the "standard" way of thinking about regression. In Bayes world, we think about the distribution first, then the properties it has. In other words, we can think of a regression line as a normal distribution with a mean given by $y = \beta_0 + \beta_1 x$:

$$y_i \sim N(\beta_0 + \beta_1 x_i, \sigma)$$

So, for a given data point `i`, we can calculate its likelihood as:

$$L(y_i | \beta_0, \beta_1, \sigma) \sim N(\beta_0 + \beta_1 x_i, \sigma)$$

So, in `nimble`, we'll be writing the likelihood of each data point as

`y[i] ~ dnorm(beta0_hat + beta1_hat * x[i], sigma_hat)`

which is the `nimble` equivalent of the equation immediately above this.

In practice, we often split this up so it reads more easily (which can be important for complex models):

`y[i] ~ dnorm(y_mean[i], sigma_hat)`

`y_mean[i] <- beta0_hat + beta1_hat * x[i]`

Note also that we use `~` for indicating a value "is distributed as" and `<-` for when something is not. When do we use `~`? Simply put, when something occurs in the "data" list, it can only appear on the left-hand side of an equation, and it can only be followed by a `~`, then one of the `d...()` functions like `dnorm()`.


## Writing the `nimble` model
Now, we'll write out our `nimble` model. The code consists of a statement about the likelihood of each datum, plus the prior distributions for each parameter. These include `beta0_hat`, `beta1_hat`, and `sigma_hat`.

A very funny thing about this kind of code is that the order in which things are written does not matter... the software knows where to go to get what it needs. In other words, we could write:

`y[i] ~ dnorm(y_mean[i], sigma_hat)`

`y_mean[i] <- beta0_hat + beta1_hat * x[i]`

or

`y_mean[i] <- beta0_hat + beta1_hat * x[i]`

`y[i] ~ dnorm(y_mean[i], sigma_hat)`

and the result would be the same. For clarity though, it is usually best to keep "like" things together (e.g., likelihoods for data points together and likelihoods for priors together).

We write the model inside a function called `nimbleCode()`.

```{r}
code <- nimbleCode({

	# likelihood of each datum
	for (i in 1:N) {

		# note the "sd = " part... if we leave that out, dnorm() uses tau (1 / sd)
		y[i] ~ dnorm(y_mean[i], sd = sigma_hat)
		y_mean[i] <- beta0_hat + beta1_hat * x[i]

	}

	# likelihoods of priors
	# beta0_hat and beta1_hat: "broad" normals
	#    important: use argument "sd = "
	# sigma_hat: uniform >0 (after Gelman 2006 Bayesian Analysis)

	beta0_hat ~ dnorm(0, sd = 10)
	beta1_hat ~ dnorm(0, sd = 10)
	sigma_hat ~ dunif(0, 10)

})
```

## Process the model code
The [nimble manual](https://r-nimble.org/html_manual/cha-welcome-nimble.html) describes an easy, "one-line MCMC" function. We will not use that here because as you become an advanced Bayesian modeler, there are parts of the model setup that you will want to modify. The one-line function doesn't allow that easily.

This step checks the code for errors and does other arcane things. Note that this is where we supply the three lists we created.

```{r}
model <- nimbleModel(
	code = code, # our model
	constants = constants, # constants
	data = data, # data
	inits = inits, # initialization values
	check = TRUE, # any errors?
	calculate = FALSE
)
```

You probably got a warning, "This model is not fully initialized. . . To see which variables are not initialized, use model$initializeInfo()." So, let's do that.

```{r}
model$initializeInfo()
```

Welcome to Bayesland.

## Checking for errors
This next step is optional, but it can help to check for errors before running the model. This is useful because it can take a long time to "run" the model before an error stops it.

```{r}
model$calculate()
```

This is the log-likelihood of the model. We would have received something else (i.e., a non-numerical value), plus maybe messages, had there been errors.

## Creating the MCMC confugration for the model
We now create an MCMC "configuration" for the model. This creates a sampler, which we can later adjust to get more stable results.

We also need to tell it what variables to keep track of. In our case, we are interested in estimates of `beta0_hat`, `beta1_hat` and `sigma_hat`. Keeping track of a parameter uses memory, and in some cases we use intermediate variables in which we are not interested. An example of this is our `y_mean` value in the code.

```{r}
monitors <- c('beta0_hat', 'beta1_hat', 'sigma_hat')

conf <- configureMCMC(
	model,
	monitors = monitors,
	print = TRUE,
	enableWAIC = FALSE # Watanabe's AIC... useful, but takes time
)
```

The output indicates we are monitoring the variables we said to monitor. It also indicates that a random walk (RW) sampler is used for `sigma_hat`, and conjugate samplers for the `beta`s.

If we wanted to change samplers, we could do that in this step. But for this simple problem, these are good. We would want to change samplers to make model estimates more stable, or to make the MCMC run more efficiently.

Lastly, we must "build" and compile the model. Compiling the model creates code in C, which runs much faster than R code. However, compiling can take a long time (hours). So, if you are developing a model and want to simply test that it works, it can help to subset your data (e.g., use the first ten data points) to reduce the compilation time.
```{r}
build <- buildMCMC(conf)
compiled <- compileNimble(model, build, showCompilerOutput = FALSE)
```

## Run the MCMC sampler for the model

The last step is running the MCMC sampler using `runMCMC()`. This actually "implements" the model (estimates coefficients). We need to provide the function:

* The total number of MCMC iterations (we'll use 1100 because the problem of fitting a regression line to linear data is simple--in some cases you may need to use many thousands or even millions).

* The number of burn-in iterations to exclude before "recording" MCMC samples begins (100 in this example--the number of iterations includes the burn-in, so in our case we'll get 1000 estimates of each coeffiecnt in our final output).

* The chains, or number of times to run the model. We'll use 4, which is fairly standard (you could go down to 2 in special cases, and usually more than 4 is overkill).

* The "thinning" rate, or how often to record coefficient estimates across MCMC iterations. The default is 1, meaning we record each MCMC step. But if our chains mix slowly, or coefficients are correlated, recording each step takes a lot of memory for not a lot of new information gained. So we might want to increase the thinning rate so as not to include so many values. To get a sufficent sample, we would also need then to increase the total number of iterations and burn-in iterations.
```{r}
chains <- runMCMC(
	compiled$build,
	niter = 1100, # iterations
	nburnin = 100, # burn-in
	thin = 1, # thinning rate
	nchains = 4, # numbe of chains
	inits = inits, # initialization values
	progressBar = TRUE,
	samplesAsCodaMCMC = TRUE, # for using coda packagge for plots
	summary = TRUE, # calculate summaries across chains
	WAIC = FALSE, # Watanabe's AIC... useful, but takes time
	perChainWAIC = FALSE
)
```

What did we get? The output of `runMCMC()` is a `list` object. We can peer at this list using `str()`:

```{r}
str(chains, 2)
```

This list has two main components. The first is named "`samples`", and it has one item per chain we ran (4 chains, in our example). Each chain is a matrix with 1000 rows (one per MCMC iteration), and three columns (one each for `beta0_hat`, `beta1_hat`, and `sigma_hat`).

The second main component of the output is names `summary`, and it reports the mean, standard deviation, etc. for each of the coefficients for each chain, then for all chains together. This last summary is called `all.chains` in the object. Let's take a look.

```{r}
chains$summary$all.chains
```

These are our estimates for the parameters of interest. Usually, we use the "`Mean`" to report central tendency, and the lower and upper 95% confidene interval bounds to report uncertainty.  How well did we do?
```{r, echo = TRUE}
beta0
beta1
sigma
```

You will get slightly different estimates because of the random nature of MCMC.

# Model diagnostics

It's *always* a good idea to examine model diagnostics. We'll do that here graphically using the **`coda`** and **`bayesplot`** packages.

## Chain mixing and convergence

Before we look at the parameter estimates, we should see how well the MCMC sampler actually sampled the available parameter space and if it tended to converge between and within the 4 chains. Chains should "mix" well, meaning they should sample the same portions of parameter space. To assess that, we use "trace" plots, which graph the estimated value of each coefficient for each MCMC iteration. What we want to see is what's been described as "grass, as seen from a bug's perspective".
```{r, fig.width=7.5}
mcmc_trace(chains$samples)
```

These chains look fairly good. But (at least on my case), you can see the one for `sigma_hat` looks a little different than the ones for the `beta`s. Peering at it, you can see that the chains mix a bit more slowly. This indicates that the coefficient estimates in each chain are highly correlated with values immediately preceding them. The chain is mixing more slowly as a result. The problem with this is that the chain can linger is a particular part of parameter space for too long of a time, providing biased estimates of parameters.

This issue is better highlighted using the `mcmc_trace_highlight()` method of plotting chains.
```{r, fig.width=7.5}
mcmc_trace_highlight(chains$samples, highlight =1)
```

This graph highlights estimates from chain 1 versus the other chains. You can readily see that this chain looks less well-mixed for `sigma_hat` compared to the `beta`s. The same holds for the other chains (not shown--but you can show yourself!). In this case, the `sigma_hat` chains are mixing slower, but they are still mixing well enough for a good estimate given how many samples we're taking (1000 iterations).

When you have a lot of coefficients, examining their trace plots can be tedious (and subjective). Alternatively, you can calculate the "Gelman-Rubin" statistic, which is also called $\hat{R}$ ("R-hat"). $\hat{R}$ measures the ratio of variation among the chains. It's ideal value for a well-mixed model is 1, an in practice, values of <1.1 are acceptible. The $\hat{R}$ statistic is almost always reported in publications using MCMC.

```{r}
gelman.diag(chains$samples)
```

In my case, the $\hat{R}$ values are all very close to 1, suggesting that the chains mixed well. This may be a little surprising given what we've seen, but it suggests that the `sigma_hat` chains were still mixing adequately. $\hat{R}$ measures how far apart the chains started from where they ended up.

Each chain should provide an independent estimate of coefficients. If you see that a chain has a wildly different density, there may be something amiss. You can often fix this by:

* Running the model again (it's stochastic, after all!);

* Using different initialization values;

* Changing the samplers (using the `configureMCMC()` function, above);

* Running the model for more iterations (plus maybe changing the thinning rate so you don't fill up on memory); and/or

* Changing the underlying model components (e.g., different distributions, especially favoring those that use conjugacy; use less-vague priors).

Note that in the limit of infinite samples, every MCMC chain should gravitate to the "true" posterior distribution (fix #1, above), and that some of the other fixes may actually make the run slower... so brute force may be in order. This means you may need to run your model for a few days/weeks.

If failure to mix is persistent, you may have a dimensionality/identifability issue and so need to reformulate your model (i.e., maybe use fewer coefficients or reduce correlation between predictors).

## Parameter estimates
Let's examine the posterior estimates of our model coefficients.
```{r, fig.width=7.5}
# densities of estimates by chain
mcmc_dens_overlay(chains$samples)

# densities across all chains
mcmc_dens(chains$samples)
```

We can also graphically compare the posterior distributions to their "true" values.
```{r, fig.width=7.5}
truth <- c(beta0, beta1, sigma)
mcmc_recover_intervals(chains$samples, true = truth, point_est = 'mean')
```

Here, thick part of each line represents the inner 50%CI, and the thin part the inner 90% CI. Circles represent the mean across the posterior. Our estimates are not exactly spot-on… but recall that we used a random number generator to create the data, and so the real values of the coefficients are not exactly what we set them to be.

Finally, we can efficiently examine trace plots and posterior densities using "combo" plots:

```{r, fig.width=7.5}
mcmc_combo(chains$samples)
```

### The easier way
Note that we could have gotten a regression simply by using `lm()` to do a "standard" linear regression:
```{r}
model <- lm(y ~ x)
coefficients(model)
sigma(model)
```

Note that the estimates for the `beta`s and `sigma` are nearly the same! It also finishes faster. Later, we will see how Bayesian models can be better than "standard" frequentist models when data and models are complex.  However, please keep in mind that life is too short to always be Bayesian.

*Finis!*
